{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "interpreter": {
      "hash": "36255955261d5e753ab71561dcd885ee1b72d92e8be276c61d3593b517160cb0"
    },
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit ('steel_defects': venv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "2_pipeline.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ivlucky/freelance_steel_defects/blob/master/2_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ae0DG9urf3f"
      },
      "source": [
        "# Magic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiPPh1iare9U"
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ya_WKRgDOT1P"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb9QfEfPOTm0"
      },
      "source": [
        "!pip install xmltodict==0.12.0 --quiet\n",
        "!pip install pytorch_lightning==1.4.9 --quiet\n",
        "!pip install segmentation-models-pytorch==0.2.0 --quiet\n",
        "!pip install albumentations==1.1.0 --quiet\n",
        "!pip install tensorboard==2.7.0 # for monitoring training"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54WXlZ00OSwl"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woWlp-g0VVFq"
      },
      "source": [
        "import os\n",
        "import xmltodict\n",
        "import numpy as np\n",
        "from typing import Dict\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import torch\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import pytorch_lightning as pl\n",
        "import segmentation_models_pytorch as smp\n",
        "import albumentations as albu\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import itertools"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evQn_kltOwtj"
      },
      "source": [
        "# Mount drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkATC5q-Ov9m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddf6fe61-09f1-444c-ea98-8f394a383069"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cu3WIcBJPC8e"
      },
      "source": [
        "# Global variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdprA3n3O1F2",
        "outputId": "66d6d076-4e3b-4b91-cc16-dc1c95696a37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "DATAFOLDER = \"/content/drive/MyDrive/data/Project_steel_plate_defect_detection_dataset\"\n",
        "INPUTFOLDER = os.path.join(DATAFOLDER, \"input\")\n",
        "ANNOFOLDER = os.path.join(INPUTFOLDER, \"anno\")\n",
        "IMAGESFOLDER = os.path.join(INPUTFOLDER, \"images\")\n",
        "AUGMENTEDFOLDER = os.path.join(DATAFOLDER, \"augmented\")\n",
        "PROCESSEDFOLDER = os.path.join(DATAFOLDER, \"processed\")\n",
        "MASKFOLDER = os.path.join(PROCESSEDFOLDER, \"masks\")\n",
        "SPLITFOLDER = os.path.join(DATAFOLDER, \"split\")\n",
        "OUTPUTFOLDER = os.path.join(DATAFOLDER, \"output\")\n",
        "LOGFOLDER = os.path.join(OUTPUTFOLDER, \"logs\")\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "VALSIZE = 0.2\n",
        "TESTSIZE = 0.1\n",
        "\n",
        "ENCODER = 'resnet34'\n",
        "ENCODER_WEIGHTS = 'imagenet'\n",
        "NUMCLASSES = 1\n",
        "# Note: don't use activation in model level because of process stability\n",
        "ACTIVATION = None#'sigmoid' # could be None for logits or 'softmax2d' for multicalss segmentation\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = torch.device('cuda:0')\n",
        "else:\n",
        "    DEVICE = torch.device('cpu')\n",
        "DEVICE"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FRGOszbRHc-"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCOcyXfbPw7E"
      },
      "source": [
        "def get_xmldict(xmlfile):\n",
        "\n",
        "    with open(xmlfile, 'r') as fp:\n",
        "\n",
        "        xmlcontent = fp.read()\n",
        "        xmldict = xmltodict.parse(xmlcontent)\n",
        "\n",
        "    return xmldict\n",
        "\n",
        "def set_bndbox(mask, bndbox):\n",
        "\n",
        "    xmin, xmax = int(bndbox['xmin']), int(bndbox['xmax'])\n",
        "    ymin, ymax = int(bndbox['ymin']), int(bndbox['ymax'])\n",
        "\n",
        "    mask[xmin:xmax, ymin:ymax] = 1\n",
        "\n",
        "def set_allbndbox(mask, objects):\n",
        "\n",
        "    for obj in objects:\n",
        "        set_bndbox(mask, obj['bndbox'])\n",
        "\n",
        "def get_mask(xmlfile):\n",
        "\n",
        "    xmldict = get_xmldict(xmlfile)\n",
        "\n",
        "    mask = np.zeros((200, 200))\n",
        "    if isinstance(xmldict['annotation']['object'], Dict):\n",
        "        set_bndbox(mask, xmldict['annotation']['object']['bndbox'])\n",
        "    else:\n",
        "        set_allbndbox(mask, xmldict['annotation']['object'])\n",
        "\n",
        "    return mask.T\n",
        "\n",
        "def show_image(imgfile):\n",
        "\n",
        "    img = get_image(imgfile)\n",
        "    print(img.shape)\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(14, 14))\n",
        "    ax.imshow(img)\n",
        "    ax.axis('off')\n",
        "    return fig, ax\n",
        "\n",
        "def show_mask(xmlfile, fig, ax):\n",
        "\n",
        "    mask = get_mask(xmlfile)\n",
        "    ax.imshow(mask, alpha=0.3)\n",
        "\n",
        "def show_obj(object_val, datafolder='./'):\n",
        "\n",
        "    imgfile = os.path.join(datafolder, 'images', object_val + '.jpg')\n",
        "    xmlfile = os.path.join(datafolder, 'anno', object_val + '.xml')\n",
        "\n",
        "    fig, ax = show_image(imgfile)\n",
        "    show_mask(xmlfile, fig, ax)\n",
        "\n",
        "def sets_describe(set_1, set_2):\n",
        "\n",
        "    print(f\"set_1 len: {len(set_1)}\")\n",
        "    print(f\"set_2 len: {len(set_2)}\")\n",
        "    print(f\"set_1 - set_2 len: {len(set_1 - set_2)}\")\n",
        "    print(f\"set_2 - set_1 len: {len(set_2 - set_1)}\")\n",
        "    print(f\"symmetric diff len: {len(set_2.symmetric_difference(set_1))}\")\n",
        "    print(f\"intersection len: {len(set_1.intersection(set_2))}\")\n",
        "    print(f\"union len: {len(set_1.union(set_2))}\")\n",
        "\n",
        "def get_image(imgfile):\n",
        "\n",
        "    return mpimg.imread(imgfile)\n",
        "\n",
        "def save_mask(maskfile, mask):\n",
        "\n",
        "  np.save(maskfile, mask)\n",
        "\n",
        "def load_mask(maskfile):\n",
        "\n",
        "  return np.load(maskfile)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs4c5SI09gbP"
      },
      "source": [
        "# Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRMagL3T6aEr"
      },
      "source": [
        "class SegDataset(torch.utils.data.Dataset):\n",
        "  \"\"\"Initial Dataset for segmentation images.\n",
        "  \n",
        "  Args:\n",
        "  \n",
        "      obj_list (list): list of object for this dataset\n",
        "      images_dir (str): path to images folder\n",
        "      masks_dir (str): path to segmentation masks folder\n",
        "  \n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self, obj_list, img_dir, mask_dir):\n",
        "\n",
        "    self.obj_list = obj_list\n",
        "    self.img_dir = img_dir\n",
        "    self.mask_dir = mask_dir\n",
        "  \n",
        "  def __getitem__(self, i):\n",
        "      \n",
        "    # read data\n",
        "    if i > len(self.obj_list):\n",
        "        raise KeyError('i not in self.obj_list')\n",
        "    if i < 0:\n",
        "        raise KeyError('i is negative')\n",
        "\n",
        "    obj = self.obj_list[i]\n",
        "    imgfile = os.path.join(self.img_dir, '.'.join([obj, 'jpg']))\n",
        "    maskfile = os.path.join(self.mask_dir, '.'.join([obj, 'npy']))\n",
        "    \n",
        "    image = get_image(imgfile)\n",
        "    mask = load_mask(maskfile)\n",
        "        \n",
        "    return image, mask, obj\n",
        "      \n",
        "  def __len__(self):\n",
        "    return len(self.obj_list)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmCZWYUm9kOq"
      },
      "source": [
        "class SegDataModule(pl.LightningDataModule):\n",
        "  \"\"\"Initial DataModule for segmentation images.\n",
        "    \n",
        "    Args:\n",
        "    \n",
        "        anno_dir (str): dir with annotations\n",
        "        img_dir: (str): dir with images\n",
        "        mask_dir (str): dir with mask\n",
        "        val_size (float): part for \n",
        "        test_size=0.1,\n",
        "        batch_size=32,\n",
        "        random_state=42\n",
        "    \n",
        "  \n",
        "  \"\"\"\n",
        "  def __init__(self, \n",
        "               anno_dir: str = \"./anno\",\n",
        "               img_dir: str = \"./images\",\n",
        "               mask_dir: str = \"./masks\",\n",
        "               val_size=0.2,\n",
        "               test_size=0.1,\n",
        "               batch_size=32,\n",
        "               random_state=42):\n",
        "\n",
        "    super().__init__()\n",
        "    self.anno_dir = anno_dir\n",
        "    self.img_dir = img_dir\n",
        "    self.mask_dir = mask_dir\n",
        "    self.val_size = val_size\n",
        "    self.test_size = test_size\n",
        "    self.batch_size = batch_size\n",
        "    self.random_state = random_state\n",
        "\n",
        "  def prepare_data(self):\n",
        "\n",
        "    jpgfiles = sorted(os.listdir(self.anno_dir))\n",
        "    xmlfiles = sorted(os.listdir(self.img_dir))\n",
        "\n",
        "    set_1 = set([f.split('.')[0] for f in jpgfiles])\n",
        "    set_2 = set([f.split('.')[0] for f in xmlfiles])\n",
        "    common_objects = set_1.intersection(set_2)\n",
        "\n",
        "    os.makedirs(self.mask_dir, exist_ok=True)\n",
        "\n",
        "    for obj in tqdm(common_objects):\n",
        "\n",
        "      xmlfile = os.path.join(self.anno_dir, \".\".join([obj, \"xml\"]))\n",
        "      mask = get_mask(xmlfile)\n",
        "      maskfile = os.path.join(self.img_dir, \".\".join([obj, \"npy\"]))\n",
        "      save_mask(maskfile, mask)\n",
        "\n",
        "  def setup(self):\n",
        "\n",
        "    jpgfiles = sorted(os.listdir(self.anno_dir))\n",
        "    xmlfiles = sorted(os.listdir(self.img_dir))\n",
        "\n",
        "    set_1 = set([f.split('.')[0] for f in jpgfiles])\n",
        "    set_2 = set([f.split('.')[0] for f in xmlfiles])\n",
        "    common_objects = set_1.intersection(set_2)\n",
        "    defect_types = set([obj.split('_') for obj in common_objects])\n",
        "\n",
        "    # split data per defects\n",
        "    def split(defect_type):\n",
        "\n",
        "      defect_objects = [obj for obj in common_objects if defect_type in obj]\n",
        "\n",
        "      nontrain_part = 1 - (self.val_size+self.test_size)\n",
        "      train, val = train_test_split(defect_objects,\n",
        "                                    test_size=nontrain_part,\n",
        "                                    random_state=self.random_state)\n",
        "      \n",
        "      val, test = train_test_split(val, \n",
        "                                   test_size=self.test_size/nontrain_part,\n",
        "                                   random_state=self.random_state)\n",
        "      \n",
        "      return train, val, test\n",
        "\n",
        "    self.train = []\n",
        "    self.val = []\n",
        "    self.test = []\n",
        "\n",
        "    for defect_type in defect_types:\n",
        "      train, dev, test = split(defect_type)\n",
        "\n",
        "      self.train.append(train)\n",
        "      self.dev.append(dev)\n",
        "      self.test.append(test)\n",
        "\n",
        "    self.train = list(itertools.chain.from_iterable(self.train))\n",
        "    self.traindataset = SegDataset(self.train, self.img_dir, self.mask_dir)\n",
        "\n",
        "    self.val = list(itertools.chain.from_iterable(self.val))\n",
        "    self.valdataset = SegDataset(self.val, self.img_dir, self.mask_dir)\n",
        "\n",
        "    self.test = list(itertools.chain.from_iterable(self.test))\n",
        "    self.testdataset = SegDataset(self.test, self.img_dir, self.mask_dir)\n",
        "\n",
        "    self.alldataset = SegDataset(self.train+self.val+self.test,\n",
        "                                 self.img_dir, self.mask_dir)\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(self.traindataset, batch_size=self.batch_size)\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(self.valdataset, batch_size=self.batch_size)\n",
        "\n",
        "  def test_dataloader(self):\n",
        "    return DataLoader(self.testdataset, batch_size=self.batch_size)\n",
        "\n",
        "  def predict_dataloader(self):\n",
        "    return DataLoader(self.alldataset)\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kl2A8DEBE5vE"
      },
      "source": [
        "class PlSegModel(pl.LightningModule):\n",
        "  \"\"\"\n",
        "  Some code is from https://github.com/Shreeyak/pytorch-lightning-segmentation-template\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, model, loss, \n",
        "      train_metric, val_metric, test_metric,\n",
        "      sample=None,\n",
        "    ):\n",
        "\n",
        "    super().__init__()\n",
        "    self.model = model\n",
        "\n",
        "    self.loss = loss\n",
        "\n",
        "    self.train_metric = train_metric\n",
        "    self.val_metric = val_metric\n",
        "    self.test_metric = test_metric\n",
        "    self.sample = sample\n",
        "    self.monitor_imgs = self.sample\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "      In lightning, forward defines the prediction/inference actions.\n",
        "      This method can be called elsewhere in the LightningModule with: `outputs = self(inputs)`.\n",
        "    \"\"\"\n",
        "      \n",
        "    output_mask = self.model(x)\n",
        "    return output_mask\n",
        "  \n",
        "  def training_step(self, batch, batch_idx):\n",
        "    \"\"\"\n",
        "      Defines the train loop. It is independent of forward().\n",
        "      Don’t use any cuda or .to(device) calls in the code. \n",
        "      PL will move the tensors to the correct device.\n",
        "    \"\"\"\n",
        "\n",
        "    inputs, mask, objects = batch\n",
        "    outputs = self(inputs)\n",
        "\n",
        "    pred = outputs\n",
        "    target = mask\n",
        "    loss = self.loss(pred, target.type_as(pred))\n",
        "    self.train_metric(pred, target)\n",
        "    \n",
        "    self.log(f'train_metric', self.train_metric, \n",
        "             on_step=True, on_epoch=True, \n",
        "             prog_bar=False, logger=True, sync_dist=True)\n",
        "    self.log(f'train_loss', loss, \n",
        "             on_step=True, on_epoch=True, \n",
        "             prog_bar=False, logger=True, sync_dist=True)\n",
        "\n",
        "    return loss\n",
        "  \n",
        "  def validation_step(self, batch, batch_idx):\n",
        "      \n",
        "    inputs, mask, labels, objects = batch\n",
        "    outputs = self(inputs)\n",
        "      \n",
        "    pred = outputs\n",
        "    target = mask\n",
        "      \n",
        "    loss = self.loss(pred, target.type_as(pred))\n",
        "    self.val_metric(pred, target)\n",
        "\n",
        "    self.log(f'val_metric', self.val_metric,\n",
        "              on_step=True, on_epoch=True, \n",
        "              prog_bar=False, logger=True, sync_dist=True)\n",
        "    self.log(f'val_loss', loss,\n",
        "              on_step=True, on_epoch=True,\n",
        "              prog_bar=False, logger=True, sync_dist=True)\n",
        "  \n",
        "  def test_step(self, batch, batch_idx):\n",
        "      \n",
        "    inputs, mask, labels, objects = batch\n",
        "    outputs = self(inputs)\n",
        "\n",
        "    pred = outputs\n",
        "    target = mask\n",
        "    loss = self.loss(pred, target.type_as(pred))\n",
        "    self.test_metric(pred, target)\n",
        "\n",
        "    self.log(f'test_metric', self.test_metric,\n",
        "              on_step=True, on_epoch=True,\n",
        "              prog_bar=False, logger=True, sync_dist=True)\n",
        "    self.log(f'test_loss', loss,\n",
        "              on_step=True, on_epoch=True,\n",
        "              prog_bar=False, logger=True, sync_dist=True)\n",
        "  \n",
        "  def configure_optimizers(self):\n",
        "\n",
        "      optimizer = torch.optim.Adam(self.parameters(), lr=1e-4)\n",
        "      lr_scheduler = ReduceLROnPlateau(optimizer, patience=3, verbose=True)\n",
        "      \n",
        "      sched = {'scheduler': lr_scheduler,\n",
        "                'monitor': f'val_metric'}\n",
        "      \n",
        "      return [optimizer], [sched]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AYthOgT9cbz"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7FNTa20lRen"
      },
      "source": [
        "# create segmentation model with pretrained encoder\n",
        "model = smp.Unet(\n",
        "    encoder_name=ENCODER,        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
        "    encoder_weights=ENCODER_WEIGHTS,     # use `imagenet` pre-trained weights for encoder initialization\n",
        "    classes=NUMCLASSES,   # model output channels (number of classes in your dataset)\n",
        "    in_channels=1,\n",
        "    activation=ACTIVATION\n",
        ")"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7AZx3njOS1I"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}